{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e74a6af-a319-4ad9-82d6-46579a07f664",
   "metadata": {},
   "source": [
    "# SARSA and Q-Learning Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff7ff5b1-1a67-4523-887c-d25e93fc3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "from enum import IntEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28c514f-5120-42b0-b668-f3ab611bf6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movement(IntEnum):\n",
    "    UP = 1\n",
    "    RIGHT = 2\n",
    "    DOWN = 3\n",
    "    LEFT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef6cf8-472e-4086-8fb0-fc76fb696b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld2D():\n",
    "    def __init__(self, \n",
    "                 width:int, \n",
    "                 height:int, \n",
    "                 start:tuple[int, int], \n",
    "                 goal:tuple[int, int], \n",
    "                 obstacles:list[tuple[int, int]]):\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        if start[0] < 0 or start[0] > width - 1:\n",
    "            raise ValueError('Invalid starting point x coordinate. Should be integer in range of [0, self.width).')\n",
    "        if start[1] < 0 or start[1] > height - 1:\n",
    "            raise ValueError('Invalid starting point y coordinate. Should be integer in range of [0, self.height).')\n",
    "        if goal[0] < 0 or goal[0] > width - 1:\n",
    "            raise ValueError('Invalid goal point x coordinate. Should be integer in range of [0, self.width).')\n",
    "        if goal[1] < 0 or goal[1] > height - 1:\n",
    "            raise ValueError('Invalid goal point y coordinate. Should be integer in range of [0, self.height).')\n",
    "\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.state = self.start\n",
    "        \n",
    "        self.obstacles = obstacles\n",
    "        self.QTable = np.zeros((width, height, 4))\n",
    "\n",
    "    def reset():\n",
    "        self.state = self.start\n",
    "        self.QTable = np.zeros((width, height, 4))\n",
    "        \n",
    "    def step(self, action: int):\n",
    "        x, y = self.state\n",
    "        \n",
    "        if action == Movement.RIGHT:\n",
    "            x = max(x+1, self.width-1)\n",
    "        elif action == Movement.LEFT:\n",
    "            x = min(x-1, 0)\n",
    "        elif action == Movement.UP:\n",
    "            y = max(y-1, 0)\n",
    "        elif action == Movement.DOWN:\n",
    "            y = max(y+1, self.height-1)\n",
    "        \n",
    "        next_state = (x,y)\n",
    "        reward = -1\n",
    "        end = False\n",
    "\n",
    "        if next_state in obstacles:\n",
    "            reward = -100\n",
    "            end = True\n",
    "        elif next_state == self.goal:\n",
    "            reward = 100\n",
    "            end = True\n",
    "\n",
    "        return next_state, reward, end\n",
    "\n",
    "    def epsilon_greedy_policy(self, state: tuple[int, int], epsilon: float):\n",
    "        if epsilon >= 1 or epsilon <= 0:\n",
    "            raise ValueError('Invalid epsilon value. Should be in range of (0,1).')\n",
    "        \n",
    "        exploration_next_action = np.random.randint(4)+1\n",
    "        exploitation_next_action = np.argmax(self.QTable[state[0], state[1]])\n",
    "        \n",
    "        return exploration_next_action if np.random.uniform() <= epsilon else exploitation_next_action\n",
    "    \n",
    "    def SARSA(self, episodes: int, alpha:float, gamma:float, epsilon:float):\n",
    "        self.reset()\n",
    "        for i in range(episodes):\n",
    "            self.state = self.start\n",
    "            action = self.epsilon_greedy_policy(self.state, epsilon)\n",
    "            end = False\n",
    "            while not end:\n",
    "                next_state, reward, end = self.step(action)\n",
    "                next_action = self.epsilon_greedy_policy(next_state, epsilon)\n",
    "                self.QTable[self.state[0], self.state[1], action] += alpha * (reward + gamma*self.QTable[next_state[0],next_state[1],next_action] - self.QTable[self.state[0],self.state[1],action])\n",
    "\n",
    "                self.state = next_state\n",
    "                action = next_action\n",
    "        \n",
    "    def QLearning(self, episodes: int, alpha:float, gamma:float, epsilon:float):\n",
    "        self.reset()\n",
    "        for i in range(episodes):\n",
    "            self.state = self.start\n",
    "            end = False\n",
    "            while not end:\n",
    "                next_state, reward, end = self.step(action)\n",
    "                action = self.epsilon_greedy_policy(self.state, epsilon)\n",
    "                self.QTable[self.state[0], self.state[1], action] += alpha * (reward + gamma*np.max(self.QTable[next_state[0],next_state[1]) - self.QTable[self.state[0],self.state[1],action])\n",
    "\n",
    "                self.state = next_state\n",
    "                action = next_action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
